{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import imutils\n",
    "import math\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the 2 images\n",
    "A = cv2.imread(\"./assets/A.png\", cv2.IMREAD_COLOR)\n",
    "B = cv2.imread(\"./assets/B.png\", cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the input images\n",
    "cv2.namedWindow(\"A\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"A\", 700, 700)\n",
    "cv2.imshow(\"A\", A)\n",
    "cv2.namedWindow(\"B\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"B\", 700, 700)\n",
    "cv2.imshow(\"B\", B)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for aligning the 2 images based using the ORB Algorithm which is a feature based keypoint detection algorithm. \n",
    "def align_images(image, template, maxFeatures=100, keepPercent=0.1):\n",
    "    \n",
    "    imageGray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    templateGray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Create the keypoint for both the images.\n",
    "    orb = cv2.ORB_create(maxFeatures)\n",
    "    (kpsA, descsA) = orb.detectAndCompute(imageGray, None)\n",
    "    (kpsB, descsB) = orb.detectAndCompute(templateGray, None)\n",
    "    \n",
    "    # Matching of the features from both images \n",
    "    method = cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING\n",
    "    matcher = cv2.DescriptorMatcher_create(method)\n",
    "    matches = matcher.match(descsA, descsB, None)\n",
    "    \n",
    "    # Sorting of the matches based on the distance. Shorter distance indicates more similarity. \n",
    "    matches = sorted(matches, key=lambda x:x.distance)\n",
    "    \n",
    "    # Keeping top 'keepPercent'% matches. This value (passed as funtion argument) was obtained experimentally for \n",
    "    #the given images. \n",
    "    keep = int(len(matches) * keepPercent)\n",
    "    matches = matches[:keep]\n",
    "    \n",
    "    #Visualizing the matched keypoints.\n",
    "    matched_pt = cv2.drawMatches(image, kpsA, template, kpsB, matches, None)\n",
    "    matched_pt = imutils.resize(matched_pt, width=1000)\n",
    "    cv2.imshow(\"Matched Keypoints\", matched_pt)\n",
    "    cv2.waitKey(0)\n",
    "   \n",
    "    #Creating the Homography Matrix for alignment of the images using Warp Perspective.\n",
    "    ptsA = np.zeros((len(matches), 2), dtype=\"float\")\n",
    "    ptsB = np.zeros((len(matches), 2), dtype=\"float\")\n",
    "    \n",
    "\n",
    "    for (i, m) in enumerate(matches):\n",
    "        ptsA[i] = kpsA[m.queryIdx].pt\n",
    "        ptsB[i] = kpsB[m.trainIdx].pt\n",
    "   \n",
    "    (H, mask) = cv2.findHomography(ptsA, ptsB, method=cv2.RANSAC)\n",
    "        \n",
    "    aligned = cv2.warpPerspective(image, H, template.shape[:2])\n",
    "\n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = B.copy()\n",
    "template = A.copy()\n",
    "\n",
    "aligned = align_images(image, template, maxFeatures=500, keepPercent=0.17) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned = imutils.resize(aligned, width=700)\n",
    "template = imutils.resize(template, width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizing the alignment by overlaying the image A and the aligned image. \n",
    "#(The aligned image is the image B.png after aligning it with A.png)\n",
    "overlay = template.copy()\n",
    "output = aligned.copy()\n",
    "cv2.addWeighted(overlay, 0.5, output, 0.5, 0, output)\n",
    "\n",
    "cv2.imshow(\"Overlay of the Aligned Images\", output)\n",
    "\n",
    "#For comparison, the original images overlayed with each other are also visualised.\n",
    "X = A.copy()\n",
    "Y = B.copy()\n",
    "cv2.addWeighted(Y, 0.5, X, 0.5, 0, Y)\n",
    "\n",
    "cv2.namedWindow(\"Overlay of the Original Images\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Overlay of the Original Images\", 700, 700)\n",
    "cv2.imshow(\"Overlay of the Original Images\", Y)\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for coorecting for the environmental factors. \n",
    "#The Dark Channel Prior (DCP) algorithm is used to correct the haze and fog. \n",
    "\n",
    "def env_correction(img_n, img, kernel_size = 5, omega = 10, tx = 0.5 ):        # 'img_n' represents normalized form \n",
    "                                                                       # of the original image 'img'\n",
    "    \n",
    "    # Finding the Dark Chaneel with a 10x10 kernel. \n",
    "    b,g,r = cv2.split(img_n)\n",
    "    dc = cv2.min(cv2.min(r,g),b)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(kernel_size,kernel_size))\n",
    "    dark = cv2.erode(dc,kernel)\n",
    "    \n",
    "    [h,w] = img_n.shape[:2]\n",
    "    img_size = h*w\n",
    "    numpx = int(max(math.floor(img_size/1000),1))\n",
    "    darkvec = dark.reshape(img_size)\n",
    "    imvec = img_n.reshape(img_size,3)\n",
    "\n",
    "    # Sorting the dark channel pixels\n",
    "    indices = darkvec.argsort()\n",
    "    indices = indices[img_size-numpx::]\n",
    "    \n",
    "    # Claculating the atmospheric light\n",
    "    atmsum = np.zeros([1,3])\n",
    "    for ind in range(1,numpx):\n",
    "        atmsum = atmsum + imvec[indices[ind]]\n",
    "    A_light = atmsum / numpx\n",
    "    \n",
    "    # Normalizing the image with the atmospheric light.\n",
    "    im3 = np.empty(img_n.shape,img_n.dtype)\n",
    "    for ind in range(0,3):\n",
    "        im3[:,:,ind] = img_n[:,:,ind]/A_light[0,ind]\n",
    "\n",
    "    transmission = 1 - omega*dark      # Estimation of the transmission using the dark channel.\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    gray = np.float64(gray)/255\n",
    "    r = 100\n",
    "    eps = 0.1\n",
    "    \n",
    "    # Computing the guided filter for transmission estimation\n",
    "    mean_I = cv2.boxFilter(gray,cv2.CV_64F,(r,r))\n",
    "    mean_p = cv2.boxFilter(transmission, cv2.CV_64F,(r,r))\n",
    "    mean_Ip = cv2.boxFilter(gray*transmission,cv2.CV_64F,(r,r))\n",
    "    cov_Ip = mean_Ip - mean_I*mean_p\n",
    "\n",
    "    mean_II = cv2.boxFilter(gray*gray,cv2.CV_64F,(r,r))\n",
    "    var_I   = mean_II - mean_I*mean_I;\n",
    "\n",
    "    a = cov_Ip/(var_I + eps);\n",
    "    b = mean_p - a*mean_I;\n",
    "\n",
    "    mean_a = cv2.boxFilter(a,cv2.CV_64F,(r,r));\n",
    "    mean_b = cv2.boxFilter(b,cv2.CV_64F,(r,r));\n",
    "\n",
    "    t = mean_a*gray + mean_b;\n",
    "    \n",
    "    res = np.empty(img_n.shape,img_n.dtype);\n",
    "    t = cv2.max(t,tx);                      # Clipping the transmission to the min. value\n",
    "    \n",
    "    # Finding the corrected image after application of transmission and atmospheric light.\n",
    "    for ind in range(0,3):\n",
    "        res[:,:,ind] = (img_n[:,:,ind]-A_light[0,ind])/t + A_light[0,ind]\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of the image. \n",
    "A_n = A.astype('float64')/255;\n",
    "aligned_n = aligned.astype('float64')/255;\n",
    "\n",
    "A_corrected = env_correction(A_n, A, kernel_size = 10, omega = 10, tx = 0.6)\n",
    "A_corrected = cv2.resize(A_corrected, (700, 700))\n",
    "aligned_corrected = env_correction(aligned_n, aligned, kernel_size = 8, omega = 10, tx = 0.6)\n",
    "\n",
    "# A_corrected = A.astype('float32');\n",
    "# aligned_corrected = A.astype('float32');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the original and corrected version of A.png\n",
    "\n",
    "cv2.namedWindow(\"A\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"A\", 700, 700)\n",
    "cv2.imshow(\"A\", A)\n",
    "cv2.namedWindow(\"A_corrected\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"A_corrected\", 700, 700)\n",
    "cv2.imshow(\"A_corrected\", A_corrected)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the original and corrected version of the aligned B.png\n",
    "\n",
    "cv2.namedWindow(\"aligned\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"aligned\", 700, 700)\n",
    "cv2.imshow(\"aligned\", aligned)\n",
    "cv2.namedWindow(\"aligned_corrected\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"aligned_corrected\", 700, 700)\n",
    "cv2.imshow(\"aligned_corrected\", aligned_corrected)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the both the corrected images.\n",
    "\n",
    "cv2.namedWindow(\"A_corrected\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"A_corrected\", 700, 700)\n",
    "cv2.imshow(\"A_corrected\", A_corrected)\n",
    "cv2.namedWindow(\"aligned_corrected\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"aligned_corrected\", 700, 700)\n",
    "cv2.imshow(\"aligned_corrected\", aligned_corrected)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the corrected images to gray scale for further processing for finding the differences. \n",
    "gray_A = cv2.cvtColor(A_corrected.astype('float32'), cv2.COLOR_BGR2GRAY)\n",
    "gray_aligned = cv2.cvtColor(aligned_corrected.astype('float32'), cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the gray scale versions of the images\n",
    "\n",
    "cv2.namedWindow(\"gray_A\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"gray_A\", 700, 700)\n",
    "cv2.imshow(\"gray_A\", gray_A)\n",
    "cv2.namedWindow(\"gray_aligned\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"gray_aligned\", 700, 700)\n",
    "cv2.imshow(\"gray_aligned\", gray_aligned)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the differences using a series of morphological operations. \n",
    "\n",
    "kernel = np.ones((11, 11), np.uint8)\n",
    "A_dilation = cv2.dilate(gray_A, kernel, iterations=1)\n",
    "aligned_dilation = cv2.dilate(gray_aligned, kernel, iterations=1)\n",
    "\n",
    "# A_erode = cv2.erode(gray_A, kernel, iterations=1)\n",
    "# aligned_erode = cv2.erode(gray_aligned, kernel, iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"aligned_dilation\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"aligned_dilation\", 700, 700)\n",
    "cv2.imshow(\"aligned_dilation\", aligned_dilation)\n",
    "cv2.namedWindow(\"A_dilation\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"A_dilation\", 700, 700)\n",
    "cv2.imshow(\"A_dilation\", A_dilation)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = cv2.subtract(aligned_dilation, A_dilation)\n",
    "\n",
    "#Displaying the crude difference between the Images.\n",
    "cv2.imshow(\"Difference Between the 2 Images\", diff)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (diff * 255).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholding the difference image and finding the contours to mark the differences. \n",
    "\n",
    "ret, thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_TOZERO_INV + cv2.THRESH_OTSU)\n",
    "# thresh = cv2.medianBlur(thresh,5)\n",
    "kernel = np.ones((11, 11), np.uint8)\n",
    "thresh = cv2.erode(thresh, kernel, iterations=1)      #The erosion and dilation together performing opening of the image.\n",
    "thresh = cv2.dilate(thresh, kernel, iterations=1)\n",
    "\n",
    "cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imshow(\"Threshold\", thresh)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_marked = A_corrected.copy()\n",
    "B_marked = aligned_corrected.copy()\n",
    "\n",
    "# Marking the differences with boxes from the obtained contours.\n",
    "for c in cnts:\n",
    "    (x, y, w, h) = cv2.boundingRect(c)\n",
    "    cv2.rectangle(A_marked, (x, y), (x + w, y + h), (0, 255, 255), 1)\n",
    "    cv2.rectangle(B_marked, (x, y), (x + w, y + h), (0, 255, 255), 2)\n",
    "\n",
    "stacked = np.hstack([A_marked, B_marked])\n",
    "cv2.imshow(\"Marked differences between A and B\", stacked)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_n_A = A_corrected.copy()\n",
    "pic_n_B = aligned_corrected.copy()\n",
    "\n",
    "\n",
    "# reshaping the images into 2D array for application of the k-means clustering algo.  \n",
    "pic_n_A = pic_n_A.reshape(A_corrected.shape[0]*A_corrected.shape[1], A_corrected.shape[2])\n",
    "pic_n_B = pic_n_B.reshape(aligned_corrected.shape[0]*aligned_corrected.shape[1], aligned_corrected.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying k-means classifier to label or mark the pixels into k no. of classes. (Here, k = 3)\n",
    "kmeans_A = sklearn.cluster.KMeans(n_clusters=3, n_init=\"auto\", random_state=0).fit(pic_n_A)\n",
    "pic_n_A = kmeans_A.cluster_centers_[kmeans_A.labels_]\n",
    "kmeans_B = sklearn.cluster.KMeans(n_clusters=3, n_init=\"auto\", random_state=0).fit(pic_n_B)\n",
    "pic_n_B = kmeans_B.cluster_centers_[kmeans_B.labels_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again reshaping the clustered images. \n",
    "cluster_pic_A = pic_n_A.reshape(A_corrected.shape[0], A_corrected.shape[1], A_corrected.shape[2])\n",
    "cluster_pic_B = pic_n_B.reshape(aligned_corrected.shape[0], aligned_corrected.shape[1], aligned_corrected.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the images after k-means clustering.\n",
    "cv2.imshow(\"cluster_pic_A\", cluster_pic_A)\n",
    "cv2.imshow(\"cluster_pic_B\", cluster_pic_B)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 700, 3)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_corrected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Segmenting the image based on the labels assigned to each pixel by the k-means clustering algorithm. \n",
    "cluster_pic_Ac = aligned_corrected.copy()\n",
    "i=0\n",
    "for x in range(0,700):\n",
    "        for y in range(0,700):\n",
    "            for c in range(1):\n",
    "                if (kmeans_A.labels_[i] == 0 ):                     \n",
    "                    cluster_pic_Ac[x,y] = [0,0,255]             # b g r\n",
    "                elif (kmeans_A.labels_[i] == 1 ):                     \n",
    "                    cluster_pic_Ac[x,y] = [90,0,100]             # b g r\n",
    "                elif (kmeans_A.labels_[i] == 2 ):                     \n",
    "                    cluster_pic_Ac[x,y] = [0,255,0]           # b g r\n",
    "                else:\n",
    "                    cluster_pic_Ac[x,y] = [0,0,0]\n",
    "                    pass\n",
    "                i = i+1\n",
    "cv2.imshow(\"cluster_pic_Ac\", cluster_pic_Ac)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholding to highlight the Segmented changes. \n",
    "z = cluster_pic_Ac.copy()\n",
    "for x in range(0,700):\n",
    "        for y in range(0,700):\n",
    "            for c in range(1):\n",
    "                if (thresh[x,y]*255 < 100 ):                    \n",
    "                    z[x,y] = [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding classification legends to the output image and marking the contours around them for better visualization.\n",
    "fontScale = 0.5\n",
    "thickness = 2\n",
    "color = (255,255,255)\n",
    "\n",
    "for c in cnts:\n",
    "    (x, y, w, h) = cv2.boundingRect(c)\n",
    "    cv2.rectangle(z, (x, y), (x + w, y + h), (255, 255, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the marked differences and the segmented classifications. \n",
    "cv2.imshow(\"Marked Differences\", stacked)\n",
    "cv2.imshow(\"Segmented Differences\", z)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
